{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pickle\nimport cv2\nimport numpy as np\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-04T12:02:52.835918Z","iopub.execute_input":"2023-06-04T12:02:52.836346Z","iopub.status.idle":"2023-06-04T12:02:52.874498Z","shell.execute_reply.started":"2023-06-04T12:02:52.836309Z","shell.execute_reply":"2023-06-04T12:02:52.872687Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Load the saved model from the pickle file\nfrom keras.models import load_model\nmodel = load_model('/kaggle/input/resnet-50-h5-model/ResNet50_fine_tuning.h5')","metadata":{"execution":{"iopub.status.busy":"2023-06-04T12:02:52.876856Z","iopub.execute_input":"2023-06-04T12:02:52.877248Z","iopub.status.idle":"2023-06-04T12:03:00.471546Z","shell.execute_reply.started":"2023-06-04T12:02:52.877216Z","shell.execute_reply":"2023-06-04T12:03:00.469962Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the path to the folder containing the images to classify\nfolder_path = '/kaggle/input/custom-dataset/custom-data'","metadata":{"execution":{"iopub.status.busy":"2023-06-04T12:03:00.474853Z","iopub.execute_input":"2023-06-04T12:03:00.475826Z","iopub.status.idle":"2023-06-04T12:03:00.481150Z","shell.execute_reply.started":"2023-06-04T12:03:00.475746Z","shell.execute_reply":"2023-06-04T12:03:00.479732Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class_labels = ['c0 - safe driving', 'c1 - texting with right hand', 'c2 - talking on the phone with right hand',\n                'c3 - texting with left hand', 'c4 - talking on the phone with the right hand', \n                'c5 - operating the radio', 'c6 - drinking', 'c7 - reaching behind', 'c8 - doing hair and makeup',\n                'c9 - talking to passengers']","metadata":{"execution":{"iopub.status.busy":"2023-06-04T12:03:00.483108Z","iopub.execute_input":"2023-06-04T12:03:00.483544Z","iopub.status.idle":"2023-06-04T12:03:00.499381Z","shell.execute_reply.started":"2023-06-04T12:03:00.483504Z","shell.execute_reply":"2023-06-04T12:03:00.497749Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Open a file to write the output\nwith open('custom_dataset_output.txt', 'w') as out_file:\n    # Loop through all images in the folder and classify them\n    for filename in os.listdir(folder_path):\n        # Load the image and preprocess it for the model\n        img = cv2.imread(os.path.join(folder_path, filename))\n        img = cv2.resize(img, (224, 224))\n        img = np.expand_dims(img, axis=0)\n\n        # Use the model to make a prediction\n        pred = model.predict(img)\n\n        # Get the predicted class label\n        pred_label = class_labels[np.argmax(pred)]\n\n        # Print the predicted class\n        out_file.write(f'Image {filename} is classified as class {pred_label}\\n')\n        #out_file.write(f'Image {filename} is classified as class {np.argmax(pred)}\\n')","metadata":{"execution":{"iopub.status.busy":"2023-06-04T12:03:22.023690Z","iopub.execute_input":"2023-06-04T12:03:22.024244Z","iopub.status.idle":"2023-06-04T12:03:27.802516Z","shell.execute_reply.started":"2023-06-04T12:03:22.024204Z","shell.execute_reply":"2023-06-04T12:03:27.800831Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 192ms/step\n1/1 [==============================] - 0s 166ms/step\n1/1 [==============================] - 0s 169ms/step\n1/1 [==============================] - 0s 179ms/step\n1/1 [==============================] - 0s 177ms/step\n1/1 [==============================] - 0s 166ms/step\n1/1 [==============================] - 0s 164ms/step\n1/1 [==============================] - 0s 173ms/step\n1/1 [==============================] - 0s 179ms/step\n1/1 [==============================] - 0s 169ms/step\n1/1 [==============================] - 0s 181ms/step\n1/1 [==============================] - 0s 164ms/step\n1/1 [==============================] - 0s 166ms/step\n1/1 [==============================] - 0s 173ms/step\n1/1 [==============================] - 0s 179ms/step\n1/1 [==============================] - 0s 169ms/step\n1/1 [==============================] - 0s 162ms/step\n1/1 [==============================] - 0s 177ms/step\n1/1 [==============================] - 0s 176ms/step\n1/1 [==============================] - 0s 170ms/step\n1/1 [==============================] - 0s 170ms/step\n","output_type":"stream"}]}]}